{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIOps Incident Management_AlertsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abasu644/dataanalysis/blob/master/AIOps_Incident_Management_AlertsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNKaJz5j_ylj",
        "colab_type": "text"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJR6t_gCQe_x",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV",
        "colab_type": "text"
      },
      "source": [
        "### Install and Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab_type": "code",
        "outputId": "dbfc0cf9-e49b-4c4a-b4bd-9827dd494597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"Importing packages\")\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Importing packages\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab_type": "code",
        "outputId": "3752c71e-29ac-44c0-fc18-e3e7af6e626d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "!pip install https://github.com/abasu644/transformers/archive/master.zip\n",
        "!pip  install pytorch-nlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/abasu644/transformers/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/abasu644/transformers/archive/master.zip\n",
            "\u001b[K     | 1.0MB 419kB/s\n",
            "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): transformers==2.1.1 from https://github.com/abasu644/transformers/archive/master.zip in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.17.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2019.8.19)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.1.83)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.0.35)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.2 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (1.13.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (7.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.2->boto3->transformers==2.1.1) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.2->boto3->transformers==2.1.1) (0.15.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.1.1-cp36-none-any.whl size=312921 sha256=0cffea9e946278c1acd149bd55ae940a04fec721af0069cd2aded9fee9ddbf3a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e1vg71yi/wheels/84/c1/bb/9eabb2cdf1c1310b2c9b19bc5443784d13476c0dd8f3e54475\n",
            "Successfully built transformers\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (0.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2018.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->pytorch-nlp) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok002ceNB8E7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "265f9057-4771-4d31-bad1-f4b78f779637"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification ,BertForNextSentencePrediction\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqG7FzRVFEIv",
        "colab_type": "text"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab_type": "code",
        "outputId": "17240ca9-cdfe-4113-dad9-428d720c8b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc",
        "colab_type": "text"
      },
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTU8Awqv4Eln",
        "colab_type": "text"
      },
      "source": [
        "Incident Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnBHKD8BdcEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f35f147f-7fd4-4c52-8115-641708664185"
      },
      "source": [
        "!git clone https://github.com/abasu644/dataanalysis.git\n",
        "!unzip dataanalysis/data/opis_events_Jan2019_Oct2019_incident.zip\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'dataanalysis' already exists and is not an empty directory.\n",
            "Archive:  dataanalysis/data/opis_events_Jan2019_Oct2019_incident.zip\n",
            "  inflating: opis_events_Jan2019_Oct2019/events_2019_01.csv  \n",
            "  inflating: opis_events_Jan2019_Oct2019/events_2019_03.csv  \n",
            "  inflating: opis_events_Jan2019_Oct2019/events_2019_04.csv  \n",
            "  inflating: opis_events_Jan2019_Oct2019/events_2019_05.csv  \n",
            "  inflating: opis_events_Jan2019_Oct2019/events_2019_06.csv  \n",
            "  inflating: opis_events_Jan2019_Oct2019/events_2019_07.csv  \n",
            "  inflating: opis_events_Jan2019_Oct2019/events_2019_08.csv  \n",
            "  inflating: opis_events_Jan2019_Oct2019/events_2019_09.csv  \n",
            "  inflating: opis_events_Jan2019_Oct2019/events_2019_10.csv  \n",
            "  inflating: OPIS_incident_data.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAgvLlKIguuT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b5067a8e-11b4-4ec9-a688-a0240ab15b9f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "## Importing Textblob package\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Importing CountVectorizer for sparse matrix/ngrams frequencies\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "## Import datetime\n",
        "import datetime as dt\n",
        "import nltk.compat\n",
        "import itertools\n",
        "import chardet\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import glob\n",
        "import re\n",
        "\n",
        "def replace_http_txt(s,tx):\n",
        "  try:\n",
        "    return(re.sub(r\"http\\S+\", \"\", s[tx]))\n",
        "  except:\n",
        "    return \" \"\n",
        "  \n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpEPx3QB7IS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "path = r'opis_events_Jan2019_Oct2019' # use your path\n",
        "all_files = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "li = []\n",
        "\n",
        "for filename in all_files:\n",
        "    dftmp = pd.read_csv(filename, index_col=None, header=0)\n",
        "    li.append(dftmp)\n",
        "\n",
        "dfi = pd.concat(li, axis=0, ignore_index=True)\n",
        "\n",
        "dfi=dfi[[\"text\",\"title\"]]\n",
        "\n",
        "dfi=dfi.drop_duplicates()\n",
        "df_pos=dfi.copy()\n",
        "\n",
        "df_neg=dfi.copy()\n",
        "df_neg.index=np.random.permutation(df_neg.index)\n",
        "df_neg=df_pos.merge(df_neg, left_index=True,right_index=True) \n",
        "df_neg=df_neg[['text_x','title_y']]\n",
        "df_neg.columns=[\"text\",\"title\"]\n",
        "df_pos['Next_Sent']=1\n",
        "df_neg['Next_Sent']=0\n",
        "dftrain=pd.concat([df_pos,df_neg],axis=0)\n",
        "dftrain['sent']=dftrain.apply(lambda s: \"CLSTOKEN \" + re.sub(r\"http\\S+\", \"\", s['title'] )+ \n",
        "                      \" SEPTOKEN \" + replace_http_txt(s,'text' )  + \" SEPTOKEN \",axis=1 )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1XH7J2iU_et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDLeOEwnYnvn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0188e860-38db-451e-b9fa-72717dceaaea"
      },
      "source": [
        "# Data preparation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "from textblob import Word\n",
        "import re\n",
        "MAX_LEN=128\n",
        "\n",
        "def get_bert_data(data):\n",
        "      stop = stopwords.words(\"english\")\n",
        "      stop.extend(['Comment','subscriptions','Edit','To','Monitor','manage','Alert','Triggered','EMAIL','EXTERNAL','click'])\n",
        "      def prepare_data_snp(df):\n",
        "        #Start Pars\n",
        "        df[\"sent\"] = df[\"sent\"].str.replace(\"[^\\w\\s]\",\" \")\n",
        "        df[\"sent\"] = df[\"sent\"].str.replace(\"\\n\",\" \")\n",
        "        df[\"sent\"] = df[\"sent\"].str.replace(\"\\t\",\" \")\n",
        "        df[\"sent\"] = df[\"sent\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "        df[\"sent\"] = df[\"sent\"].apply(lambda x: \" \".join([Word(myword).lemmatize() for myword in x.split()])  )\n",
        "        def filterToken(token_txt):\n",
        "          token_txt=word_tokenize(token_txt)\n",
        "          finaltokenized=[]\n",
        "          for token in token_txt:\n",
        "            def removeSystemtoken(s):\n",
        "              d=l=u=0\n",
        "              for c in s:\n",
        "                  if c.isdigit():\n",
        "                      d=d+1\n",
        "                  elif c.isalpha():\n",
        "                      l=l+1\n",
        "                  else:\n",
        "                      u=u+1\n",
        "              return d,l,u\n",
        "            digc,letc,uc=removeSystemtoken(token)\n",
        "            if  digc >1 and letc>0 :\n",
        "              continue\n",
        "            elif digc >0 and letc==0 :\n",
        "              continue \n",
        "            elif digc ==0 and letc==0 and uc>0:\n",
        "              continue\n",
        "            else:\n",
        "              finaltokenized.append(token)\n",
        "          return (' ').join(finaltokenized)\n",
        "        df[\"sent\"] =  df[\"sent\"].apply(lambda x: filterToken(x))\n",
        "        df[\"sent\"] = df[\"sent\"].str.replace(\"CLSTOKEN\",\"[CLS]\")\n",
        "        df[\"sent\"] = df[\"sent\"].str.replace(\"SEPTOKEN\",\"[SEP]\")\n",
        "        return df.sent.values\n",
        "\n",
        "      dt=prepare_data_snp(data.sample(frac=1))\n",
        "      tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "      tokenized_texts = [tokenizer.tokenize(sent)[:MAX_LEN] for sent in dt]\n",
        "\n",
        "      # Convert token to vocabulary indices\n",
        "      input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "      input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "      attention_masks = []\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "      \n",
        "      #get segment ids\n",
        "      segment_ids=[]\n",
        "      for idi in input_ids:\n",
        "        #print(idi.shape)\n",
        "        first_sent_len= np.where(idi==102)[0][0]\n",
        "        sec_sent_len= MAX_LEN-(first_sent_len)\n",
        "        #print(first_sent_len)\n",
        "        seg_id =first_sent_len*[0] + sec_sent_len*[1]\n",
        "        segment_ids.append(seg_id)\n",
        "      return input_ids,attention_masks,segment_ids\n",
        "    \n",
        "    \n",
        "    \n",
        "input_ids,attention_masks,segment_ids=get_bert_data(dftrain)\n",
        "labels=dftrain.Next_Sent.values"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 942206.04B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct",
        "colab_type": "text"
      },
      "source": [
        "## Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw5K2A5Ko1RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "train_seg_masks, validation_seg_masks, _, _ = train_test_split(segment_ids, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "\n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "train_seg_masks = torch.tensor(train_seg_masks)\n",
        "validation_seg_masks = torch.tensor(validation_seg_masks)\n",
        "\n",
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size=32\n",
        "train_data = TensorDataset(train_inputs, train_masks,train_seg_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks,validation_seg_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNl8khAhPYju",
        "colab_type": "text"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxSMw0FrptiL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "682ac583-6665-4e2c-9b9d-94cfd1b9cddd"
      },
      "source": [
        "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
        "model.cuda()\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                     lr=2e-5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 73354.03B/s]\n",
            "100%|██████████| 440473133/440473133 [00:16<00:00, 26105013.50B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab_type": "code",
        "outputId": "53054e20-878d-4a81-ad3f-b24507395148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 5\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    \n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "   \n",
        "    b_input_ids, b_input_mask,b_seg_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    inputs = {'input_ids':       batch[0],\n",
        "                      'attention_mask':  batch[1],\n",
        "                      'token_type_ids': b_seg_mask,\n",
        "                      'next_sentence_label':   batch[3]}\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=b_seg_mask, attention_mask=b_input_mask,next_sentence_label=b_labels)\n",
        "    #loss= model(**inputs)\n",
        "    loss=loss[0]\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "!mkdir -p aiops\n",
        "model.save_pretrained(\"aiops\")\n",
        "!zip aiops.zip aiops/*\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  20%|██        | 1/5 [04:09<16:39, 249.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.7805353495581396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [08:18<12:28, 249.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.6955694501427399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [12:27<08:18, 249.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.6952716252584566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [16:35<04:09, 249.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.692970545127474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch: 100%|██████████| 5/5 [20:45<00:00, 249.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.6913820234523422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  adding: aiops/config.json (deflated 52%)\n",
            "  adding: aiops/pytorch_model.bin (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU4SnvDTnF-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    print(list(zip(pred_flat , labels_flat)))\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab_type": "code",
        "outputId": "90abfe00-b6f3-4bc4-f448-87b29e6f13aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAHwCAYAAADeojx9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3P8DUcQ/4MdW/3EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCYjlf3h2TJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "cb9a4c7a-0f94-4c86-dbae-d173a082bf62"
      },
      "source": [
        "# Validation\n",
        "  model.config.output_hidden_states=True\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask,b_seg_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      output = model(b_input_ids, token_type_ids=b_seg_mask, attention_mask=b_input_mask,next_sentence_label=b_labels)\n",
        "      logits=output[1]\n",
        "      \n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "!mkdir -p aiops\n",
        "model.save_pretrained(\"aiops\")\n",
        "!zip aiops.zip aiops/*"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 1), (0, 1), (1, 1), (1, 1), (1, 1), (0, 1), (1, 0), (1, 0), (1, 1), (1, 1), (1, 1), (0, 1), (1, 1), (0, 0), (1, 1), (0, 1), (1, 0), (1, 0), (0, 0), (1, 1), (0, 0), (1, 0), (0, 0), (1, 1), (1, 0), (0, 0), (1, 1), (1, 0), (0, 1), (0, 1), (0, 0), (1, 1)]\n",
            "[(1, 0), (1, 1), (1, 1), (0, 0), (1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1), (0, 0), (1, 0), (0, 0), (1, 1), (1, 0), (1, 0), (1, 1), (0, 1), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (1, 0), (0, 0)]\n",
            "[(1, 0), (1, 0), (0, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (1, 0), (0, 0), (0, 0), (1, 1), (0, 1), (0, 0), (1, 1), (0, 1), (1, 0), (1, 1), (1, 1), (0, 0), (1, 0), (1, 0), (1, 1), (0, 0)]\n",
            "[(1, 1), (1, 1), (0, 1), (1, 1), (1, 1), (1, 0), (0, 0), (1, 1), (1, 0), (1, 1), (0, 0), (1, 1), (1, 0), (1, 1), (1, 1), (0, 1), (1, 1), (1, 0), (1, 0), (1, 1), (1, 0), (0, 0), (1, 0), (1, 0), (1, 0), (0, 1), (1, 1), (1, 1), (1, 1), (0, 1), (1, 0), (1, 1)]\n",
            "[(0, 0), (1, 0), (1, 1), (1, 0), (1, 0), (1, 1), (0, 1), (1, 1), (0, 1), (1, 0), (0, 0), (1, 0), (0, 1), (1, 0), (1, 1), (1, 0), (0, 0), (1, 0), (0, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (0, 0), (1, 0), (1, 1), (1, 0), (1, 0), (1, 1), (1, 1)]\n",
            "[(1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (1, 0), (1, 1), (1, 0), (0, 0), (1, 1), (1, 0), (1, 0), (0, 0), (1, 1), (1, 1), (1, 1), (1, 0), (0, 0), (1, 1), (1, 1), (0, 0), (1, 0), (1, 1), (1, 1), (1, 1), (0, 0), (1, 0)]\n",
            "[(1, 1), (1, 1), (0, 0), (0, 0), (1, 0), (1, 1), (1, 1), (1, 1), (0, 1), (1, 1), (0, 1), (1, 0), (1, 0), (0, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (0, 0), (1, 1), (1, 0), (0, 1), (1, 1), (1, 1), (1, 0), (0, 1)]\n",
            "[(1, 1), (0, 1), (1, 1), (1, 0), (0, 0), (1, 1), (1, 0), (0, 0), (1, 0), (1, 0), (1, 1), (1, 1), (1, 1), (1, 0), (0, 1), (1, 0), (1, 1), (1, 0), (1, 0), (1, 1), (0, 0), (1, 1), (0, 0), (1, 0), (0, 0), (1, 1), (1, 0), (1, 0), (0, 1), (0, 0), (1, 0), (1, 0)]\n",
            "[(1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (0, 1), (1, 1), (1, 0), (0, 1), (1, 0), (1, 0), (1, 0), (0, 1), (1, 1), (1, 1), (1, 0), (1, 0), (0, 1), (0, 1), (0, 1), (0, 0), (0, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (1, 1), (0, 0), (1, 1), (0, 0)]\n",
            "[(1, 1), (0, 0), (1, 1), (1, 0), (1, 1), (0, 1), (1, 1), (1, 0), (0, 0), (1, 1), (0, 1), (0, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (0, 1), (1, 0), (0, 0), (1, 1), (0, 0), (1, 1), (0, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 0), (0, 0), (1, 0)]\n",
            "[(1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (0, 1), (0, 0), (0, 1), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1), (1, 0), (0, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (1, 0), (1, 1), (0, 0), (1, 0), (1, 0), (1, 0), (0, 1), (1, 0)]\n",
            "[(1, 0), (1, 0), (1, 1), (0, 0), (1, 0), (1, 1), (0, 0), (1, 1), (1, 0), (1, 1), (1, 1), (0, 0), (0, 0), (1, 0), (0, 1), (1, 1), (1, 1), (0, 0), (0, 1), (1, 0), (1, 0), (0, 1), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (0, 0), (0, 1), (1, 1), (1, 0), (1, 0)]\n",
            "[(0, 1), (1, 1), (0, 0), (0, 0), (1, 0), (1, 0), (1, 0), (1, 1), (0, 1), (1, 1), (1, 0), (1, 0), (1, 0), (1, 1), (0, 1), (1, 1), (1, 1), (0, 0), (1, 0), (1, 1), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (0, 0), (0, 1), (1, 0), (0, 1), (1, 1), (1, 1), (1, 1)]\n",
            "[(0, 0), (1, 1), (1, 1), (1, 0), (0, 0), (0, 1), (0, 0), (0, 1), (0, 1), (0, 0), (0, 1), (1, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (0, 0), (1, 0), (1, 0), (0, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1), (0, 0), (1, 1), (1, 0), (0, 1)]\n",
            "[(0, 0), (1, 0), (1, 1), (0, 1), (0, 1), (1, 0), (1, 0), (0, 1), (1, 1), (1, 1), (1, 1), (1, 1), (0, 0), (1, 1), (1, 1), (0, 1), (1, 1), (1, 0), (1, 1), (1, 1), (0, 1), (0, 1), (1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1)]\n",
            "[(1, 0), (1, 1), (1, 0), (1, 0), (0, 0), (1, 1), (1, 1), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (0, 0), (1, 1), (0, 1), (0, 0), (1, 1), (0, 0), (1, 1), (0, 1), (1, 1), (1, 1), (0, 0), (1, 0), (1, 1), (1, 1), (0, 0), (0, 0), (0, 0)]\n",
            "[(0, 1), (1, 1), (1, 0), (1, 1), (1, 1), (0, 1), (0, 1), (1, 1), (1, 0), (1, 0), (1, 0), (0, 1), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1), (1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (0, 0), (1, 0), (0, 1), (1, 0), (1, 1), (1, 0), (0, 0), (1, 0), (0, 0), (1, 1)]\n",
            "[(1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (0, 0), (1, 1), (1, 0), (1, 0), (0, 1), (0, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (0, 1), (1, 1), (1, 0), (1, 0), (0, 1), (1, 1), (0, 1), (1, 0), (1, 1), (1, 1)]\n",
            "[(1, 0), (1, 0), (1, 1), (1, 1), (1, 0), (1, 0), (1, 1), (1, 0), (0, 1), (1, 0), (1, 0), (0, 1), (0, 1), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (0, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 0), (1, 1), (1, 0), (0, 0), (0, 0), (1, 1), (0, 0)]\n",
            "[(0, 0), (0, 1), (1, 1), (0, 0), (0, 1), (0, 0), (1, 1), (1, 0), (1, 0)]\n",
            "Validation Accuracy: 0.5121527777777778\n",
            "updating: aiops/config.json (deflated 52%)\n",
            "updating: aiops/pytorch_model.bin (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-G03mmwH3aI",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at our training loss over all batches:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyRa-5CcHv_g",
        "colab_type": "text"
      },
      "source": [
        "## Training Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGUjadmAgcD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "50db1b41-6cd7-45f9-d180-9b5ca96c7f27"
      },
      "source": [
        "# Validation\n",
        "  modelorig = BertForNextSentencePrediction.from_pretrained(\"aiops\")\n",
        "  modelorig.cuda()\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  modelorig.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask,b_seg_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      output = modelorig(b_input_ids, token_type_ids=b_seg_mask, attention_mask=b_input_mask,next_sentence_label=b_labels)\n",
        "      logits=output[1]\n",
        "      \n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 1), (0, 1), (1, 1), (1, 1), (1, 1), (0, 1), (1, 0), (1, 0), (1, 1), (1, 1), (1, 1), (0, 1), (1, 1), (0, 0), (1, 1), (0, 1), (1, 0), (1, 0), (0, 0), (1, 1), (0, 0), (1, 0), (0, 0), (1, 1), (1, 0), (0, 0), (1, 1), (1, 0), (0, 1), (0, 1), (0, 0), (1, 1)]\n",
            "[(1, 0), (1, 1), (1, 1), (0, 0), (1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1), (0, 0), (1, 0), (0, 0), (1, 1), (1, 0), (1, 0), (1, 1), (0, 1), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (1, 0), (0, 0)]\n",
            "[(1, 0), (1, 0), (0, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (1, 0), (0, 0), (0, 0), (1, 1), (0, 1), (0, 0), (1, 1), (0, 1), (1, 0), (1, 1), (1, 1), (0, 0), (1, 0), (1, 0), (1, 1), (0, 0)]\n",
            "[(1, 1), (1, 1), (0, 1), (1, 1), (1, 1), (1, 0), (0, 0), (1, 1), (1, 0), (1, 1), (0, 0), (1, 1), (1, 0), (1, 1), (1, 1), (0, 1), (1, 1), (1, 0), (1, 0), (1, 1), (1, 0), (0, 0), (1, 0), (1, 0), (1, 0), (0, 1), (1, 1), (1, 1), (1, 1), (0, 1), (1, 0), (1, 1)]\n",
            "[(0, 0), (1, 0), (1, 1), (1, 0), (1, 0), (1, 1), (0, 1), (1, 1), (0, 1), (1, 0), (0, 0), (1, 0), (0, 1), (1, 0), (1, 1), (1, 0), (0, 0), (1, 0), (0, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (0, 0), (1, 0), (1, 1), (1, 0), (1, 0), (1, 1), (1, 1)]\n",
            "[(1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (1, 0), (1, 1), (1, 0), (0, 0), (1, 1), (1, 0), (1, 0), (0, 0), (1, 1), (1, 1), (1, 1), (1, 0), (0, 0), (1, 1), (1, 1), (0, 0), (1, 0), (1, 1), (1, 1), (1, 1), (0, 0), (1, 0)]\n",
            "[(1, 1), (1, 1), (0, 0), (0, 0), (1, 0), (1, 1), (1, 1), (1, 1), (0, 1), (1, 1), (0, 1), (1, 0), (1, 0), (0, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (0, 0), (1, 1), (1, 0), (0, 1), (1, 1), (1, 1), (1, 0), (0, 1)]\n",
            "[(1, 1), (0, 1), (1, 1), (1, 0), (0, 0), (1, 1), (1, 0), (0, 0), (1, 0), (1, 0), (1, 1), (1, 1), (1, 1), (1, 0), (0, 1), (1, 0), (1, 1), (1, 0), (1, 0), (1, 1), (0, 0), (1, 1), (0, 0), (1, 0), (0, 0), (1, 1), (1, 0), (1, 0), (0, 1), (0, 0), (1, 0), (1, 0)]\n",
            "[(1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (0, 1), (1, 1), (1, 0), (0, 1), (1, 0), (1, 0), (1, 0), (0, 1), (1, 1), (1, 1), (1, 0), (1, 0), (0, 1), (0, 1), (0, 1), (0, 0), (0, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (1, 1), (0, 0), (1, 1), (0, 0)]\n",
            "[(1, 1), (0, 0), (1, 1), (1, 0), (1, 1), (0, 1), (1, 1), (1, 0), (0, 0), (1, 1), (0, 1), (0, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (0, 1), (1, 0), (0, 0), (1, 1), (0, 0), (1, 1), (0, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 0), (0, 0), (1, 0)]\n",
            "[(1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (0, 1), (0, 0), (0, 1), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1), (1, 0), (0, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (1, 0), (1, 1), (0, 0), (1, 0), (1, 0), (1, 0), (0, 1), (1, 0)]\n",
            "[(1, 0), (1, 0), (1, 1), (0, 0), (1, 0), (1, 1), (0, 0), (1, 1), (1, 0), (1, 1), (1, 1), (0, 0), (0, 0), (1, 0), (0, 1), (1, 1), (1, 1), (0, 0), (0, 1), (1, 0), (1, 0), (0, 1), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (0, 0), (0, 1), (1, 1), (1, 0), (1, 0)]\n",
            "[(0, 1), (1, 1), (0, 0), (0, 0), (1, 0), (1, 0), (1, 0), (1, 1), (0, 1), (1, 1), (1, 0), (1, 0), (1, 0), (1, 1), (0, 1), (1, 1), (1, 1), (0, 0), (1, 0), (1, 1), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (0, 0), (0, 1), (1, 0), (0, 1), (1, 1), (1, 1), (1, 1)]\n",
            "[(0, 0), (1, 1), (1, 1), (1, 0), (0, 0), (0, 1), (0, 0), (0, 1), (0, 1), (0, 0), (0, 1), (1, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (0, 0), (1, 0), (1, 0), (0, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1), (0, 0), (1, 1), (1, 0), (0, 1)]\n",
            "[(0, 0), (1, 0), (1, 1), (0, 1), (0, 1), (1, 0), (1, 0), (0, 1), (1, 1), (1, 1), (1, 1), (1, 1), (0, 0), (1, 1), (1, 1), (0, 1), (1, 1), (1, 0), (1, 1), (1, 1), (0, 1), (0, 1), (1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1)]\n",
            "[(1, 0), (1, 1), (1, 0), (1, 0), (0, 0), (1, 1), (1, 1), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (0, 0), (1, 1), (0, 1), (0, 0), (1, 1), (0, 0), (1, 1), (0, 1), (1, 1), (1, 1), (0, 0), (1, 0), (1, 1), (1, 1), (0, 0), (0, 0), (0, 0)]\n",
            "[(0, 1), (1, 1), (1, 0), (1, 1), (1, 1), (0, 1), (0, 1), (1, 1), (1, 0), (1, 0), (1, 0), (0, 1), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1), (1, 1), (1, 0), (1, 1), (1, 0), (1, 0), (0, 0), (1, 0), (0, 1), (1, 0), (1, 1), (1, 0), (0, 0), (1, 0), (0, 0), (1, 1)]\n",
            "[(1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (0, 0), (1, 1), (1, 0), (1, 0), (0, 1), (0, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (0, 1), (1, 1), (1, 0), (1, 0), (0, 1), (1, 1), (0, 1), (1, 0), (1, 1), (1, 1)]\n",
            "[(1, 0), (1, 0), (1, 1), (1, 1), (1, 0), (1, 0), (1, 1), (1, 0), (0, 1), (1, 0), (1, 0), (0, 1), (0, 1), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0), (0, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 0), (1, 1), (1, 0), (0, 0), (0, 0), (1, 1), (0, 0)]\n",
            "[(0, 0), (0, 1), (1, 1), (0, 0), (0, 1), (0, 0), (1, 1), (1, 0), (1, 0)]\n",
            "Validation Accuracy: 0.5121527777777778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRgBNnmWg3U4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "6f2e81ab-ddd6-4042-fe1f-3277dc3c0ff7"
      },
      "source": [
        "##Check Similarity\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "dict_master_enc={}\n",
        "dfdig = pd.read_csv(filepath);\n",
        "dfdig=dfdig[[\"Description\",\"Subject\"]]\n",
        "dfdig=dfdig.drop_duplicates()\n",
        "dfdig['sent']=dfdig.apply(lambda s: \"CLSTOKEN \" + re.sub(r\"http\\S+\", \"\", s['Subject']) + \n",
        "                      \" SEPTOKEN \" +  re.sub(r\"http\\S+\", \"\", s['Description']) + \" SEPTOKEN \",axis=1 )\n",
        "\n",
        "validation_inputs, validation_masks,validation_seg_masks=get_bert_data(dfdig)\n",
        "validation_inputs=torch.tensor(validation_inputs)\n",
        "validation_masks=torch.tensor(validation_masks)\n",
        "validation_seg_masks=torch.tensor(validation_seg_masks)\n",
        "validation_sent=torch.tensor(dfdig['sent'].index)\n",
        "\n",
        "similarity_data = TensorDataset(validation_inputs, validation_masks,validation_seg_masks,validation_sent)\n",
        "similarity_data_sampler = SequentialSampler(similarity_data)\n",
        "similarity_dataloader = DataLoader(similarity_data, sampler=similarity_data_sampler, batch_size=32)\n",
        "mod_bert=model.bert\n",
        "mod_bert.eval()\n",
        "for batch in similarity_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask,b_seg_mask,b_sent = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      sequence_output= mod_bert(b_input_ids, attention_mask=b_input_mask)\n",
        "      a=sequence_output[0].detach().cpu().numpy()\n",
        "      b=b_sent.detach().cpu().numpy()\n",
        "      for j in range(len(b)):\n",
        "        dict_master_enc[b[j]]=a[j][0]\n",
        "        \n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0269397a4d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdict_master_enc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdfdig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdfdig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdfdig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Subject\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdfdig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdfdig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m dfdig['sent']=dfdig.apply(lambda s: \"CLSTOKEN \" + re.sub(r\"http\\S+\", \"\", s['Subject']) + \n",
            "\u001b[0;31mNameError\u001b[0m: name 'filepath' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-NID0gCMwk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"opis_events_Jan2019_Oct2019/events_2019_10.csv\",\"rb\") as mydata:\n",
        "    result = chardet.detect(mydata.read(1000000))\n",
        "    \n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07BI34dUCuwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_child_enc={}\n",
        "\n",
        "  \n",
        "dfdigchld = pd.read_csv(\"opis_events_Jan2019_Oct2019/events_2019_10.csv\");\n",
        "print(dfdigchld.columns)\n",
        "dfdigchld=dfdigchld[[\"title\",\"text\",\"tags\"]]\n",
        "dfdigchld=dfdigchld.drop_duplicates()\n",
        "dfdigchld['sent']=dfdigchld.apply(lambda s: \"CLSTOKEN \" + re.sub(r\"http\\S+\", \"\", s['title'] )+ \n",
        "                      \" SEPTOKEN \" + replace_http_txt(s,\"text\") + \" SEPTOKEN \",axis=1 )\n",
        "\n",
        "validation_inputs, validation_masks,validation_seg_masks=get_bert_data(dfdigchld)\n",
        "validation_inputs=torch.tensor(validation_inputs)\n",
        "validation_masks=torch.tensor(validation_masks)\n",
        "validation_seg_masks=torch.tensor(validation_seg_masks)\n",
        "validation_sent=torch.tensor(dfdigchld['sent'].index)\n",
        "\n",
        "similarity_data = TensorDataset(validation_inputs, validation_masks,validation_seg_masks,validation_sent)\n",
        "similarity_data_sampler = SequentialSampler(similarity_data)\n",
        "similarity_dataloader = DataLoader(similarity_data, sampler=similarity_data_sampler, batch_size=32)\n",
        "mod_bert=model.bert\n",
        "mod_bert.eval()\n",
        "for batch in similarity_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask,b_seg_mask,b_sent = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      sequence_output= mod_bert(b_input_ids, attention_mask=b_input_mask)\n",
        "      a=sequence_output[0].detach().cpu().numpy()\n",
        "      b=b_sent.detach().cpu().numpy()\n",
        "      for j in range(len(b)):\n",
        "        dict_child_enc[b[j]]=a[j][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG6c0NaAOt_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VvobKijDD3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tc=0\n",
        "for key in dict_child_enc.keys():\n",
        "  tc=tc+1\n",
        "  if tc ==10:\n",
        "    break\n",
        "  best_match_master=-1\n",
        "  best_match_score=0\n",
        "  for mkey in dict_master_enc.keys():\n",
        "      tmp_match_score=cosine_similarity(dict_child_enc[key].reshape(1,-1),dict_master_enc[mkey].reshape(1,-1))\n",
        "      if best_match_score < tmp_match_score:\n",
        "        best_match_score=tmp_match_score\n",
        "        best_match_master=mkey\n",
        "  print(best_match_score,dfdigchld[dfdigchld.index==key].text,dfdig[dfdig.index==best_match_master].Subject)\n",
        "  print(\"-----------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NqpH8-4CASZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mve1gi0E6NeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array(validation_seg_masks).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M690dJqiqrm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMulmF6tTOUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "!cp -r aiops ./dataanalysis\n",
        "!cd dataanalysis\n",
        "!git init .  \n",
        "!git add aiops/*    \n",
        "!git config — global user.email “abasu644@gmail.com”\n",
        "!git config — global user.name “abasu”\n",
        "!git commit -m \"First commit\"    \n",
        "!git remote add origin https://abasu644:swamigalu1$@github.com/abasu644/dataanalysis.git\n",
        "#!git remote add origin remote https://github.com/abasu644/dataanalysis    \n",
        "!git push origin master\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}